Performance of hash table:
The hash function was implemented by summing the ascii values of all characters in a word and modding by the size of the hash table. The test program is run by compiling with "make test". When run the first argument is the size of the hash table and the second is the number of random strings to put through the hash function. These strings are of random length with random characters. Testing of the function with a table size of 1,000 and one million words showed to have a trig function shape to it increasing sharply at each hundred and then dropping off. For smaller table sizes the distribution is more even but the larger the table the more of the sinusoidal pattern is seen. When the table size was tested at 10,000 I noticed a complete drop off, all chosen zero times, of indexes chosen at around index 2,500. The test program outputs to test.txt in tab delimited format. I graphed the results in Google sheets to get a clear view of what was going on. The number of times an index was chosen represents the length of the chain.

Implementation of the hash table:
I chose to implement the hash table with multiple templates. It looks weird when it is initialized because I wanted the user of the hash table to be able to chose what data structure is used to manage each index. As long as the data structure provided has get and contains methods and is key value paired it is suitable for use with the hash function. This allows me to have things like a hash table of LLL of a hash table of trees. And in this way the performance is based more on the data structure chosen. This is because the hash function chooses the index with direct access meaning that it will always be the fastest part then the performance of the creation or accessing of the key in that index is determined by the chosen data structure. It takes about 1.1 seconds to load 5000 rows into the hash table of size 100. Most the the data is random as in not real words so for your testing I recomend that you sreach for a state name like Washington or California.

Challenges of writing this program:
The hardest part about writing this program was determining how to store the data for retrieval as an array. I opted for a hash table of dictionaries. This allowed me to hash on each keyword and then add the job to the dictionary by title to insure it was only added to the dictionary once. Then when retrieving I simply ask for the size of the dictionary create an array of that size and fill each index with the corresponding position in the dictionary. Reading these in was also difficult, the data file is in csv format and requires that there be no commas in the data in each field. This is because I don't have time to write a proper csv parser. The headers are at the top and the data was generated with mockaroo. When first parsed in the job is added to the hash table by taking every data member and all keys, hashing on that string, and inserting the job into the corresponding dictionary for that key.
